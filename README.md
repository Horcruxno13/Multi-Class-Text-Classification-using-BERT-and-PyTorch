# Multi Class Text Classification using BERT and PyTorch
## "It is the effort that counts?"

The complete report can be read here: https://docs.google.com/document/d/11egycd_80bdkuq-K6W3gFSMnfNVl9HmP8GSGOfFUxmE/edit?usp=sharing 

### Introduction
After writing hundreds of lines of code, going through a ton of documentation and sample codes, reading research papers to get to the depth of the idea, I was unable to train the model with a high enough accuracy. During my attempt to train a BERT model, I encountered challenges that led to low accuracy. Despite the suboptimal outcome, I invested a significant amount of effort to overcome these obstacles and make the model work. As a result of this process, I gained valuable knowledge and insight into the workings of all the steps involved in training a neural network and other techniques in NLP. Although the final accuracy of the model may not have met my initial expectations, the effort expended and knowledge acquired in the process of attempting to improve it was valuable. I consider this experience to be a positive one, as it served to expand my understanding of NLP and the challenges involved in working on it.

### Dataset
The dataset is a text dataset that has been created for the purpose of classification in the E-commerce domain. It is composed of approximately 27,000 samples, and has been divided into four categories: "Electronics", "Household", "Books", and "Clothing & Accessories". These categories have been chosen as they are representative of the most common types of products found on E-commerce websites, covering about 80% of the products sold online. The text data in this dataset is expected to contain information such as product descriptions, reviews, or specifications, which are typically used by customers to make purchasing decisions. By using this dataset, researchers and developers can train machine learning models to accurately classify products based on their textual descriptions, which can be a useful tool in improving the user experience and recommendation systems on E-commerce platforms. The dataset was picked up from Kaggle. 

### Conclusion 
In conclusion, the goal of the project was to train a deep learning model for sequence classification using the BERT architecture. Several attempts were made to achieve high accuracy, including changes to the learning rate, optimizer function, and sequence length. In one attempt, a different dataset was used altogether, but accuracy remained low. In the final attempt, the Catalyst library was used, which helped to streamline the training process and produced the best results. Despite the challenges faced, the project successfully demonstrated the complexity of training deep learning models and the importance of choosing the right architecture and tools for the task at hand. 

Despite not being able to achieve a high accuracy score, the effort put into training the various models was not in vain. The process of exploring different models, adjusting hyperparameters, and experimenting with different techniques provided valuable insights into the world of natural language processing. Furthermore, encountering obstacles and overcoming them taught valuable lessons about perseverance and the importance of problem-solving skills in the field. Overall, the experience was an invaluable learning opportunity and will serve as a foundation for future endeavors in the realm of NLP.

For the purposes of being honest, all the codes have been uploaded onto the GitHub repository of the project.

